{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "from copy import deepcopy\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the directory of the current script\n",
    "current_dir = os.getcwd()\n",
    "# Get the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Now you can import from the 'flexc' directory\n",
    "from flexc import FLEXC_Labels, FLEXC_Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fff441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect seeds in the cwd, if it is not there it generates new ones\n",
    "seeds_file = os.path.join(current_dir, \"seeds.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_balanced(N, x, y):\n",
    "    \n",
    "    # Group indices by class\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    class_indices = {c: np.where(y == c)[0] for c in classes}\n",
    "    \n",
    "    # Determine the number of samples per class\n",
    "    min_class_count = np.min(counts)\n",
    "    samples_per_class = min(N, min_class_count)\n",
    "    \n",
    "    x_balanced, y_balanced, x_tail, y_tail = [], [], [], []\n",
    "\n",
    "    for label, indices in class_indices.items():\n",
    "        if len(indices) < 2 or samples_per_class == 0:\n",
    "            continue  # Skip classes with insufficient samples\n",
    "    \n",
    "        indices = np.random.permutation(indices)\n",
    "        selected = indices[:samples_per_class]\n",
    "        leftover = indices[samples_per_class:]\n",
    "        \n",
    "        x_balanced.extend(x[selected])\n",
    "        y_balanced.extend(y[selected])\n",
    "        x_tail.extend(x[leftover])\n",
    "        y_tail.extend(y[leftover])\n",
    "    \n",
    "    return np.array(x_balanced), np.array(x_tail), np.array(y_balanced), np.array(y_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910a2c1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_id = 1\n",
    "max_workers = 1\n",
    "n_seeds = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a492d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"glass\" if dataset_id == 0 else \"sat_image\"\n",
    "results_dir = os.path.join(os.getcwd(), f\"results_{dataset_name}\")\n",
    "os.makedirs(results_dir, exist_ok=False)\n",
    "results_file = os.path.join(results_dir, \"results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84e7cd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 146, 'name': 'Statlog (Landsat Satellite)', 'repository_url': 'https://archive.ics.uci.edu/dataset/146/statlog+landsat+satellite', 'data_url': 'https://archive.ics.uci.edu/static/public/146/data.csv', 'abstract': 'Multi-spectral values of pixels in 3x3 neighbourhoods in a satellite image, and the classification associated with the central pixel in each neighbourhood', 'area': 'Climate and Environment', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 6435, 'num_features': 36, 'feature_types': ['Integer'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1993, 'last_updated': 'Wed Feb 14 2024', 'dataset_doi': '10.24432/C55887', 'creators': ['Ashwin Srinivasan'], 'intro_paper': None, 'additional_info': {'summary': \"The database consists of the multi-spectral values of pixels in 3x3 neighbourhoods in a satellite image, and the classification associated with the central pixel in each neighbourhood. The aim is to predict this classification, given the multi-spectral values. In the sample database, the class of a pixel is coded as a number.\\r\\n\\r\\nThe Landsat satellite data is one of the many sources of information available for a scene. The interpretation of a scene by integrating spatial data of diverse types and resolutions including multispectral and radar data, maps indicating topography, land use etc. is expected to assume significant importance with the onset of an era characterised by integrative approaches to remote sensing (for example, NASA's Earth Observing System commencing this decade). Existing statistical methods are ill-equipped for handling such diverse data types. Note that this is not true for Landsat MSS data considered in isolation (as in this sample database). This data satisfies the important requirements of being numerical and at a single resolution, and standard maximum-likelihood classification performs very well. Consequently, for this data, it should be interesting to compare the performance of other methods against the statistical approach.\\r\\n\\r\\nOne frame of Landsat MSS imagery consists of four digital images of the same scene in different spectral bands. Two of these are in the visible region (corresponding approximately to green and red regions of the visible spectrum) and two are in the (near) infra-red. Each pixel is a 8-bit binary word, with 0 corresponding to black and 255 to white. The spatial resolution of a pixel is about 80m x 80m. Each image contains 2340 x 3380 such pixels.\\r\\n\\r\\nThe database is a (tiny) sub-area of a scene, consisting of 82 x 100 pixels. Each line of data corresponds to a 3x3 square neighbourhood of pixels completely contained within the 82x100 sub-area. Each line contains the pixel values in the four spectral bands (converted to ASCII) of each of the 9 pixels in the 3x3 neighbourhood and a number indicating the classification label of the central pixel. The number is a code for the following classes:\\r\\n\\r\\nNumber\\t\\t\\tClass\\r\\n1\\t\\t\\tred soil\\r\\n2\\t\\t\\tcotton crop\\r\\n3\\t\\t\\tgrey soil\\r\\n4\\t\\t\\tdamp grey soil\\r\\n5\\t\\t\\tsoil with vegetation stubble\\r\\n6\\t\\t\\tmixture class (all types present)\\r\\n7\\t\\t\\tvery damp grey soil\\r\\n\\t\\r\\nNB. There are no examples with class 6 in this dataset.\\r\\n\\r\\nThe data is given in random order and certain lines of data have been removed so you cannot reconstruct the original image from this dataset.\\r\\n\\r\\nIn each line of data the four spectral values for the top-left pixel are given first followed by the four spectral values for the top-middle pixel and then those for the top-right pixel, and so on with the pixels read out in sequence left-to-right and top-to-bottom. Thus, the four spectral values for the central pixel are given by attributes 17,18,19 and 20. If you like you can use only these four attributes, while ignoring the others. This avoids the problem which arises when a 3x3 neighbourhood straddles a boundary.\\r\\n\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'The attributes are numerical, in the range 0 to 255.', 'citation': None}}\n",
      "           name     role         type demographic description units  \\\n",
      "0    Attribute1  Feature      Integer        None        None  None   \n",
      "1    Attribute2  Feature      Integer        None        None  None   \n",
      "2    Attribute3  Feature      Integer        None        None  None   \n",
      "3    Attribute4  Feature      Integer        None        None  None   \n",
      "4    Attribute5  Feature      Integer        None        None  None   \n",
      "5    Attribute6  Feature      Integer        None        None  None   \n",
      "6    Attribute7  Feature      Integer        None        None  None   \n",
      "7    Attribute8  Feature      Integer        None        None  None   \n",
      "8    Attribute9  Feature      Integer        None        None  None   \n",
      "9   Attribute10  Feature      Integer        None        None  None   \n",
      "10  Attribute11  Feature      Integer        None        None  None   \n",
      "11  Attribute12  Feature      Integer        None        None  None   \n",
      "12  Attribute13  Feature      Integer        None        None  None   \n",
      "13  Attribute14  Feature      Integer        None        None  None   \n",
      "14  Attribute15  Feature      Integer        None        None  None   \n",
      "15  Attribute16  Feature      Integer        None        None  None   \n",
      "16  Attribute17  Feature      Integer        None        None  None   \n",
      "17  Attribute18  Feature      Integer        None        None  None   \n",
      "18  Attribute19  Feature      Integer        None        None  None   \n",
      "19  Attribute20  Feature      Integer        None        None  None   \n",
      "20  Attribute21  Feature      Integer        None        None  None   \n",
      "21  Attribute22  Feature      Integer        None        None  None   \n",
      "22  Attribute23  Feature      Integer        None        None  None   \n",
      "23  Attribute24  Feature      Integer        None        None  None   \n",
      "24  Attribute25  Feature      Integer        None        None  None   \n",
      "25  Attribute26  Feature      Integer        None        None  None   \n",
      "26  Attribute27  Feature      Integer        None        None  None   \n",
      "27  Attribute28  Feature      Integer        None        None  None   \n",
      "28  Attribute29  Feature      Integer        None        None  None   \n",
      "29  Attribute30  Feature      Integer        None        None  None   \n",
      "30  Attribute31  Feature      Integer        None        None  None   \n",
      "31  Attribute32  Feature      Integer        None        None  None   \n",
      "32  Attribute33  Feature      Integer        None        None  None   \n",
      "33  Attribute34  Feature      Integer        None        None  None   \n",
      "34  Attribute35  Feature      Integer        None        None  None   \n",
      "35  Attribute36  Feature      Integer        None        None  None   \n",
      "36        class   Target  Categorical        None        None  None   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "5              no  \n",
      "6              no  \n",
      "7              no  \n",
      "8              no  \n",
      "9              no  \n",
      "10             no  \n",
      "11             no  \n",
      "12             no  \n",
      "13             no  \n",
      "14             no  \n",
      "15             no  \n",
      "16             no  \n",
      "17             no  \n",
      "18             no  \n",
      "19             no  \n",
      "20             no  \n",
      "21             no  \n",
      "22             no  \n",
      "23             no  \n",
      "24             no  \n",
      "25             no  \n",
      "26             no  \n",
      "27             no  \n",
      "28             no  \n",
      "29             no  \n",
      "30             no  \n",
      "31             no  \n",
      "32             no  \n",
      "33             no  \n",
      "34             no  \n",
      "35             no  \n",
      "36             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "glass_id = 42\n",
    "sat_image_id=146\n",
    "dataset = fetch_ucirepo(id=glass_id if dataset_id == 0 else sat_image_id)\n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = dataset.data.features \n",
    "y = dataset.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(dataset.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(dataset.variables) \n",
    "\n",
    "\n",
    "y = y.values.astype(int).reshape(-1)\n",
    "y_mapping = {k: i for i, k in enumerate(np.unique(y))}\n",
    "y = np.array([y_mapping[i] for i in y])\n",
    "X = X.values.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af2b51c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape: (6435, 36)\n",
      "labels shape: (6435,)\n",
      "labels: (array([0, 1, 2, 3, 4, 5]), array([1533,  703, 1358,  626,  707, 1508]))\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataset shape: {X.shape}\")\n",
    "print(f\"labels shape: {y.shape}\")\n",
    "\n",
    "print(f\"labels: {np.unique(y, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56687387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged classes into class 0: [0, 2, 5]\n",
      "labels: (array([0, 1, 2, 3]), array([4399,  703,  626,  707]))\n"
     ]
    }
   ],
   "source": [
    "class_0 = [0,1] if dataset_id == 0 else [0,2,5]\n",
    "print(f\"Merged classes into class 0: {class_0}\")\n",
    "other_classes = np.setdiff1d(np.unique(y), class_0)\n",
    "classes_mapping = {K: 0 for K in class_0} | {k: i+1 for i, k in enumerate(other_classes)}\n",
    "y = np.array([classes_mapping[i] for i in y])\n",
    "print(f\"labels: {np.unique(y, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "344700c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_with_contamination(X, y, contamination=0.1):\n",
    "    \"\"\"\n",
    "    Create a dataset with a specified contamination level.\n",
    "    \"\"\"\n",
    "\n",
    "    normal_index = y == 0\n",
    "    anomaly_index = ~normal_index\n",
    "    X_anom = X[anomaly_index]\n",
    "    Y_anom = y[anomaly_index]\n",
    "    X_normal = X[normal_index]\n",
    "    Y_normal = y[normal_index]\n",
    "\n",
    "    n_anomalies = int(len(X_normal) * contamination)\n",
    "    if n_anomalies > X_anom.shape[0]:\n",
    "        n_anomalies = X_anom.shape[0]\n",
    "        n_normal = int(n_anomalies / contamination)\n",
    "        selected_normal = np.random.choice(len(X_normal), n_normal, replace=False)\n",
    "        x = np.concatenate((\n",
    "            X_normal[selected_normal],\n",
    "            X_anom\n",
    "        ))\n",
    "        y = np.concatenate((\n",
    "            Y_normal[selected_normal],\n",
    "            Y_anom\n",
    "        ))\n",
    "        return x, y\n",
    "    \n",
    "    selected_anomalies = np.random.choice(len(X_anom), n_anomalies, replace=False)\n",
    "    x = np.concatenate((\n",
    "        X_normal,  \n",
    "        X_anom[selected_anomalies]\n",
    "    ))\n",
    "    y = np.concatenate((\n",
    "        Y_normal,  \n",
    "        Y_anom[selected_anomalies]\n",
    "    ))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ec38f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised Train set size: 1752\n",
      "Unupervised train set size: 1738\n",
      "Test set size: 752\n"
     ]
    }
   ],
   "source": [
    "x_balanced, x_tail, y_balanced, y_tail= extract_balanced(np.inf, X , y)\n",
    "\n",
    "x_train_unsup, y_train_unsup = dataset_with_contamination(\n",
    "    x_tail, y_tail, contamination=0.1\n",
    ")\n",
    "\n",
    "x_train_sup, x_test, y_train_sup, y_test = train_test_split(\n",
    "    x_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced\n",
    ")\n",
    "\n",
    "print(f\"Supervised Train set size: {x_train_sup.shape[0]}\")\n",
    "print(f\"Unupervised train set size: {x_train_unsup.shape[0]}\")\n",
    "print(f\"Test set size: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "830d1bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in supervised train set: (array([0, 1, 2, 3]), array([438, 438, 438, 438]))\n",
      "Labels in unsupervised train set: (array([0, 1, 3]), array([1580,   77,   81]))\n",
      "Labels in test set: (array([0, 1, 2, 3]), array([188, 188, 188, 188]))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Labels in supervised train set: {np.unique(y_train_sup, return_counts=True)}\")\n",
    "print(f\"Labels in unsupervised train set: {np.unique(y_train_unsup, return_counts=True)}\")\n",
    "print(f\"Labels in test set: {np.unique(y_test, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ccded",
   "metadata": {},
   "source": [
    "# Experiment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd43278",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 100\n",
    "flexc_centroids_threshold = 0.1\n",
    "flexc_labels_threshold = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "319a52db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_dataset_sizes(n_total, x_min, k):\n",
    "    \"\"\"\n",
    "    Compute dataset increments s_i such that log(cumulative size) grows linearly.\n",
    "\n",
    "    Args:\n",
    "        n_total (int): Total number of samples in the dataset.\n",
    "        x_min (int): Minimum number of samples in the first dataset.\n",
    "        k (int): Number of steps.\n",
    "\n",
    "    Returns:\n",
    "        list of int: Dataset sizes to add at each step.\n",
    "    \"\"\"\n",
    "    alpha = np.log(n_total) / (k-1)\n",
    "    i_vals = np.arange(k)\n",
    "    K_vals = np.exp(alpha * i_vals)\n",
    "    s_vals = np.diff(K_vals, prepend=0)\n",
    "    s_vals = np.round(s_vals).astype(int).tolist()\n",
    "    s_vals[0] = max(x_min, s_vals[0])  # Ensure first dataset has enough points\n",
    "    s_vals = [max(1, s) for s in s_vals]  # Ensure all increments are at least 1\n",
    "    return s_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0aedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        try:\n",
    "            import random\n",
    "            random.seed(seed)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            np.random.seed(seed)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e98e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(X, y, seed, n_blocks):\n",
    "    set_seeds(seed)\n",
    "\n",
    "    x_balanced, x_tail, y_balanced, y_tail= extract_balanced(np.inf, X , y)\n",
    "\n",
    "    x_train_unsup, y_train_unsup = dataset_with_contamination(\n",
    "    x_tail, y_tail, contamination=0.1\n",
    "    )\n",
    "\n",
    "    x_train_sup, x_test, y_train_sup, y_test = train_test_split(\n",
    "        x_balanced, y_balanced, test_size=0.3, random_state=seed, stratify=y_balanced\n",
    "    )\n",
    "    # in this case these are point per class\n",
    "    blocks_points_per_class = incremental_dataset_sizes(\n",
    "        n_total=x_train_sup.shape[0]/len(np.unique(y_train_sup)),\n",
    "        x_min=1, \n",
    "        k=n_blocks\n",
    "    )\n",
    "\n",
    "    # metrics\n",
    "    flexc_labels_accuracy = defaultdict(list)\n",
    "    flexc_labels_confusion_matrix = defaultdict(list)\n",
    "    flexc_labels_training_time = defaultdict(list)\n",
    "    flexc_labels_inference_time = defaultdict(list)\n",
    "\n",
    "    flexc_centroids_accuracy = defaultdict(list)\n",
    "    flexc_centroids_confusion_matrix = defaultdict(list)\n",
    "    flexc_centroids_training_time = defaultdict(list)\n",
    "    flexc_centroids_inference_time = defaultdict(list)\n",
    "\n",
    "    flexc_centroids_v2_accuracy = defaultdict(list)\n",
    "    flexc_centroids_v2_confusion_matrix = defaultdict(list)\n",
    "    flexc_centroids_v2_training_time = defaultdict(list)\n",
    "    flexc_centroids_v2_inference_time = defaultdict(list)\n",
    "\n",
    "    rf_accuracy = defaultdict(list)\n",
    "    rf_confusion_matrix = defaultdict(list)\n",
    "    rf_training_time = defaultdict(list)\n",
    "    rf_inference_time = defaultdict(list)\n",
    "\n",
    "    rf_self_accuracy = defaultdict(list)\n",
    "    rf_self_confusion_matrix = defaultdict(list)\n",
    "    rf_self_training_time = defaultdict(list)\n",
    "    rf_self_inference_time = defaultdict(list)\n",
    "\n",
    "    rf_mislead_accuracy = defaultdict(list)\n",
    "    rf_mislead_confusion_matrix = defaultdict(list)\n",
    "    rf_mislead_training_time = defaultdict(list)\n",
    "    rf_mislead_inference_time = defaultdict(list)\n",
    "\n",
    "    iso_accuracy = defaultdict(list)\n",
    "    iso_confusion_matrix = defaultdict(list)\n",
    "    iso_training_time = defaultdict(list)\n",
    "    iso_inference_time = defaultdict(list)\n",
    "\n",
    "\n",
    "    for i, _ in enumerate(blocks_points_per_class):\n",
    "\n",
    "        n_train_sup = np.sum(blocks_points_per_class[:i+1])\n",
    "        if i == len(blocks_points_per_class) - 1:\n",
    "            # last block is the whole dataset\n",
    "            n_train_sup = np.inf\n",
    "        x_train_sup_iter, x_discarded, y_train_sup_iter, y_discarded = extract_balanced(n_train_sup, x_train_sup, y_train_sup)\n",
    "\n",
    "        #set correct n_train_sup\n",
    "        if i == len(blocks_points_per_class) - 1:\n",
    "            # last block is the whole dataset\n",
    "            classes, counts = np.unique(y_train_sup_iter, return_counts=True)\n",
    "            n_train_sup = np.min(counts)\n",
    "        \n",
    "        x_train_sup_clean_iter = x_train_sup_iter[np.where(y_train_sup_iter != 0)]\n",
    "        y_train_sup_clean_iter = y_train_sup_iter[np.where(y_train_sup_iter != 0)]\n",
    "\n",
    "        # Initialize the models\n",
    "        flexc_labels_model = FLEXC_Labels(\n",
    "            n_estimators=n_estimators,\n",
    "            contamination=0.1,  \n",
    "            random_state=seed,\n",
    "            bootstrap=True\n",
    "        )\n",
    "        flexc_centroids_model = FLEXC_Centroids(\n",
    "            n_estimators=n_estimators,\n",
    "            contamination=0.1,  \n",
    "            random_state=seed,\n",
    "            bootstrap=True\n",
    "        )\n",
    "        flexc_centroids_v2_model = deepcopy(flexc_centroids_model)\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=seed,\n",
    "            bootstrap=True\n",
    "        )\n",
    "        rf_mislead_model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=seed,\n",
    "            bootstrap=True\n",
    "        )\n",
    "        iso_f_model = IsolationForest(\n",
    "            n_estimators=n_estimators,\n",
    "            #contamination=0.1,\n",
    "            bootstrap=True,\n",
    "            random_state=seed\n",
    "        )\n",
    "        rf_iso_model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=seed,\n",
    "            bootstrap=True\n",
    "        )\n",
    "\n",
    "        ###### FLEX_C Labels\n",
    "        t = timer()\n",
    "        flexc_labels_model.fit(x_train_unsup)\n",
    "        flexc_labels_model.inject_knowledge(x_train_sup_clean_iter, y_train_sup_clean_iter)\n",
    "        flexc_labels_training_time[f\"block:{n_train_sup}\"].append(timer() - t)\n",
    "\n",
    "        t = timer()\n",
    "        flexc_labels_pred, counts = flexc_labels_model.predict_labels(x_test)\n",
    "        flexc_labels_pred = np.nan_to_num(flexc_labels_pred, nan=0)\n",
    "        flexc_labels_inference_time[f\"block:{n_train_sup}\"].append(timer() - t)\n",
    "        \n",
    "        flexc_labels_accuracy[f\"block:{n_train_sup}\"].append(accuracy_score(y_test, flexc_labels_pred))\n",
    "        flexc_labels_confusion_matrix[f\"block:{n_train_sup}\"].append(confusion_matrix(y_test, flexc_labels_pred).tolist())\n",
    "\n",
    "        ###### FLEX_C Centroids\n",
    "        t = timer()\n",
    "        flexc_centroids_model.fit(x_train_unsup)\n",
    "        flexc_centroids_model.inject_knowledge(x_train_sup_clean_iter, y_train_sup_clean_iter, score_threshold=flexc_centroids_threshold)\n",
    "        flexc_centroids_training_time[f\"block:{n_train_sup}\"].append(timer() - t)\n",
    "\n",
    "        t = timer()\n",
    "        # alarm_norm False removes the vote from untrained classifiers\n",
    "        flexc_centroids_pred, alarm = flexc_centroids_model.predict_labels(x_test, alarm_threshold=flexc_centroids_threshold, y=y_test, alarm_norm=False)\n",
    "        flexc_centroids_pred = np.argmax(flexc_centroids_pred, axis=1)\n",
    "        flexc_centroids_inference_time[f\"block:{n_train_sup}\"].append(timer() - t)\n",
    "\n",
    "        flexc_centroids_accuracy[f\"block:{n_train_sup}\"].append(accuracy_score(y_test, flexc_centroids_pred))\n",
    "\n",
    "        flexc_centroids_confusion_matrix[f\"block:{n_train_sup}\"].append(confusion_matrix(y_test, flexc_centroids_pred).tolist())\n",
    "\n",
    "        ###### FLEX_C Centroids V2\n",
    "        t = timer()\n",
    "        flexc_centroids_v2_model.fit(x_train_unsup)\n",
    "        flexc_centroids_v2_model.inject_knowledge(x_train_sup_clean_iter, y_train_sup_clean_iter, score_threshold=flexc_centroids_threshold)\n",
    "        flexc_centroids_v2_training_time[f\"block:{n_train_sup}\"].append(timer() - t)\n",
    "\n",
    "        t = timer()\n",
    "        flexc_centroids_v2_pred, alarm = flexc_centroids_v2_model.predict_labels(x_test, alarm_threshold=flexc_centroids_threshold, y=y_test)\n",
    "        flexc_centroids_v2_pred = np.argmax(flexc_centroids_v2_pred, axis=1)\n",
    "        flexc_centroids_v2_inference_time[f\"block:{n_train_sup}\"].append(timer() - t)\n",
    "\n",
    "        flexc_centroids_v2_accuracy[f\"block:{n_train_sup}\"].append(accuracy_score(y_test, flexc_centroids_v2_pred))\n",
    "       \n",
    "        flexc_centroids_v2_confusion_matrix[f\"block:{n_train_sup}\"].append(confusion_matrix(y_test, flexc_centroids_v2_pred).tolist())\n",
    "        \n",
    "\n",
    "        ###### Isolation Forest + RF\n",
    "        t = timer()\n",
    "        iso_f_model.fit(x_train_unsup)\n",
    "        rf_iso_model.fit(x_train_sup_clean_iter, y_train_sup_clean_iter)\n",
    "        iso_training_time[f\"block:{n_train_sup}\"].append(timer() - t)\n",
    "\n",
    "        t = timer()\n",
    "        iso_pred = iso_f_model.predict(x_test)\n",
    "        outliers_mask = iso_pred == -1\n",
    "        iso_pred[~outliers_mask] = 0\n",
    "        iso_pred[outliers_mask] = rf_iso_model.predict(x_test[outliers_mask])\n",
    "        iso_inference_time[f\"block:{n_train_sup}\"].append(timer() - t)\n",
    "\n",
    "        iso_accuracy[f\"block:{n_train_sup}\"].append(accuracy_score(y_test, iso_pred))\n",
    "        \n",
    "        iso_confusion_matrix[f\"block:{n_train_sup}\"].append(confusion_matrix(y_test, iso_pred).tolist())\n",
    "        \n",
    "\n",
    "        ###### RF\n",
    "        t = timer()\n",
    "        rf_model.fit(x_train_sup_iter, y_train_sup_iter)\n",
    "        rf_training_time[f\"block:{n_train_sup}\"].append(timer() - t)\n",
    "\n",
    "        t = timer()\n",
    "        rf_pred = rf_model.predict(x_test)\n",
    "        rf_inference_time[f\"block:{n_train_sup}\"].append(timer() - t)\n",
    "\n",
    "        rf_accuracy[f\"block:{n_train_sup}\"].append(accuracy_score(y_test, rf_pred))\n",
    "        \n",
    "        rf_confusion_matrix[f\"block:{n_train_sup}\"].append(confusion_matrix(y_test, rf_pred).tolist())\n",
    "        \n",
    "\n",
    "        # semi self supervised RF\n",
    "        t = timer()\n",
    "        rf_self = deepcopy(rf_model)\n",
    "        rf_y_train_self = rf_model.predict(x_train_unsup)\n",
    "        rf_self.fit(np.concatenate([x_train_sup_iter, x_train_unsup]), np.concatenate([y_train_sup_iter, rf_y_train_self]))\n",
    "        rf_self_training_time[f\"block:{n_train_sup}\"].append(timer() - t + rf_training_time[f\"block:{n_train_sup}\"][-1])\n",
    "\n",
    "        t = timer()\n",
    "        rf_self_pred = rf_self.predict(x_test)\n",
    "        rf_self_inference_time[f\"block:{n_train_sup}\"].append(timer() - t + rf_inference_time[f\"block:{n_train_sup}\"][-1])\n",
    "\n",
    "        rf_self_accuracy[f\"block:{n_train_sup}\"].append(accuracy_score(y_test, rf_self_pred))\n",
    "        \n",
    "        rf_self_confusion_matrix[f\"block:{n_train_sup}\"].append(confusion_matrix(y_test, rf_self_pred).tolist())\n",
    "        \n",
    "\n",
    "        # mislead RF\n",
    "\n",
    "        # our method should be more resistant to this false info\n",
    "        # in real life it is difficult to ensure normal data has no faults in it\n",
    "        # but faulty data is certain \n",
    "        # the fault is happened for the company to have it categorized and collected\n",
    "\n",
    "        t = timer()\n",
    "        rf_mislead_model.fit(\n",
    "            np.concatenate([x_train_sup_clean_iter, x_train_unsup]),\n",
    "            np.concatenate([y_train_sup_clean_iter, np.zeros_like(y_train_unsup)])\n",
    "        )\n",
    "        rf_mislead_training_time[f\"block:{n_train_sup}\"].append(timer() - t)\n",
    "\n",
    "        t = timer()\n",
    "        rf_mislead_pred = rf_mislead_model.predict(x_test)\n",
    "        rf_mislead_inference_time[f\"block:{n_train_sup}\"].append(timer() - t)\n",
    "\n",
    "        rf_mislead_accuracy[f\"block:{n_train_sup}\"].append(accuracy_score(y_test, rf_mislead_pred))\n",
    "        \n",
    "        rf_mislead_confusion_matrix[f\"block:{n_train_sup}\"].append(confusion_matrix(y_test, rf_mislead_pred).tolist())\n",
    "        \n",
    "\n",
    "    return {\n",
    "        \"flexc_labels\": {\n",
    "            \"accuracy\": flexc_labels_accuracy,\n",
    "            \"confusion_matrix\": flexc_labels_confusion_matrix,\n",
    "            \"training_time\": flexc_labels_training_time,\n",
    "            \"inference_time\": flexc_labels_inference_time\n",
    "            },\n",
    "        \"flexc_centroids\": {\n",
    "            \"accuracy\": flexc_centroids_accuracy,\n",
    "            \"confusion_matrix\": flexc_centroids_confusion_matrix,\n",
    "            \"training_time\": flexc_centroids_training_time,\n",
    "            \"inference_time\": flexc_centroids_inference_time\n",
    "            },\n",
    "        \"flexc_centroids_v2\": {\n",
    "            \"accuracy\": flexc_centroids_v2_accuracy,\n",
    "            \"confusion_matrix\": flexc_centroids_v2_confusion_matrix,\n",
    "            \"training_time\": flexc_centroids_v2_training_time,\n",
    "            \"inference_time\": flexc_centroids_v2_inference_time\n",
    "            },\n",
    "        \"rf\": {\n",
    "            \"accuracy\": rf_accuracy,\n",
    "            \"confusion_matrix\": rf_confusion_matrix,\n",
    "            \"training_time\": rf_training_time,\n",
    "            \"inference_time\": rf_inference_time\n",
    "            },\n",
    "        \"rf_self\": {\n",
    "            \"accuracy\": rf_self_accuracy,\n",
    "            \"confusion_matrix\": rf_self_confusion_matrix,\n",
    "            \"training_time\": rf_self_training_time,\n",
    "            \"inference_time\": rf_self_inference_time\n",
    "            },\n",
    "        \"rf_mislead\": {\n",
    "            \"accuracy\": rf_mislead_accuracy,\n",
    "            \"confusion_matrix\": rf_mislead_confusion_matrix,\n",
    "            \"training_time\": rf_mislead_training_time,\n",
    "            \"inference_time\": rf_mislead_inference_time\n",
    "            },\n",
    "        \"iso + rf\": {\n",
    "            \"accuracy\": iso_accuracy,\n",
    "            \"confusion_matrix\": iso_confusion_matrix,\n",
    "            \"training_time\": iso_training_time,\n",
    "            \"inference_time\": iso_inference_time\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f742a2e",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a42fd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with seed 2439\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "def run_single_experiment(seed, X, y, n_blocks):\n",
    "    print(f\"Running experiment with seed {seed}\")\n",
    "    results = experiment(X, y, seed, n_blocks=n_blocks)\n",
    "    return (f\"seed:{seed}\", results)\n",
    "\n",
    "experiment_results = {}\n",
    "\n",
    "# load or generate seeds\n",
    "if os.path.exists(seeds_file):\n",
    "    with open(seeds_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        random_seeds = data.get(\"seeds\", [])\n",
    "    if len(random_seeds) != n_seeds:\n",
    "        raise ValueError(f\"Expected {n_seeds} seeds, but found {len(random_seeds)} in {seeds_file}. Please delete the file to generate new seeds.\")\n",
    "    print(\"Loaded existing seeds.\")\n",
    "else:\n",
    "    random_seeds = np.random.choice(np.arange(10000), size=n_seeds, replace=False).tolist()\n",
    "    \n",
    "    data_to_save = {\"seeds\": random_seeds}\n",
    "\n",
    "    with open(seeds_file, 'w', encoding='utf-8') as f: \n",
    "        json.dump(data_to_save, f, indent=4)\n",
    "    print(\"Generated and saved new seeds.\")\n",
    "\n",
    "n_blocks = 5 if dataset_id == 0 else 10\n",
    "\n",
    "# parallel execution of the seeds\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    run_exp = partial(run_single_experiment, X=X, y=y, n_blocks=n_blocks)\n",
    "    for key, result in executor.map(run_exp, random_seeds):\n",
    "        experiment_results[key] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2e7d4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(experiment_results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mattia_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
